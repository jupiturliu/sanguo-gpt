{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SanGuo GPT Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tr-20VkiKAdJ",
    "outputId": "2a215ac5-e54c-4a90-c43d-b439cdf0a4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-24 09:31:23--  https://raw.githubusercontent.com/naosense/Yiya/master/book/%E4%B8%89%E5%9B%BD%E6%BC%94%E4%B9%89.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1206396 (1.2M) [text/plain]\n",
      "Saving to: ‘sanguo.txt’\n",
      "\n",
      "sanguo.txt          100%[===================>]   1.15M  2.56MB/s    in 0.4s    \n",
      "\n",
      "2023-08-24 09:31:24 (2.56 MB/s) - ‘sanguo.txt’ saved [1206396/1206396]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the text for training.\n",
    "!wget https://raw.githubusercontent.com/naosense/Yiya/master/book/%E4%B8%89%E5%9B%BD%E6%BC%94%E4%B9%89.txt -O sanguo.txt\n",
    "\n",
    "# The text is encoded in GBK, let's convert it to UTF-8.\n",
    "!iconv -f GBK -t UTF-8 sanguo.txt > sanguo-utf8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gB8PfD-7Q35O"
   },
   "outputs": [],
   "source": [
    "# !sed -i '/^=/,/^=/d' sanguo-utf8.txt\n",
    "# For Mac:\n",
    "!sed -i \"\" '/^=/,/^=/d' sanguo-utf8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vouv6BohXbIN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SMgCFG8VKhhL"
   },
   "outputs": [],
   "source": [
    "with open('sanguo-utf8.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dt7eVZboKoAf",
    "outputId": "064e8ac9-0ed7-4963-c404-6893aee38000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 606051\n",
      "《三国演义》（精校版全本）作者：罗贯中\n",
      " \n",
      "\n",
      "内容简介\n",
      "\n",
      "　　《三国演义》由东汉末年黄巾起义末期开始描写，至西晋初期国家重归统一结束，以魏、蜀、吴三个政治、军事集团之间的形成演变，矛盾斗争为主线，最后\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(text)}\")   # 606051 Chinese characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hXZffPKkLwZw"
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B52ByllkL7UM",
    "outputId": "c0e0f362-c357-4dfd-e2ef-55b608b22b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary of the book - less than 4000 Chinese characters.\n",
    "# See? Chinese is not that hard - you only need to know a few thousand and\n",
    "# you'll be able to read the ancient Chinese classics!\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T7xlkjq9STu3"
   },
   "outputs": [],
   "source": [
    "# I don't plan to use a tokenizer for this.\n",
    "# IMHO, a Chinese character is not a \"letter\". Instead it's more like\n",
    "# a word or subword. So we should treat each Chinese character as a token.\n",
    "\n",
    "# Turn each character into a number (index into the chars array)\n",
    "# Map character to index.\n",
    "c2i = {ch:i for i, ch in enumerate(chars)}\n",
    "# Map index to character.\n",
    "i2c = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Given a string (sequence of characters), encode it into a sequence of indices.\n",
    "encoder = lambda s: [c2i[c] for c in s]\n",
    "# Given a sequence of indices, decode it back to the string\n",
    "decoder = lambda l: ''.join([i2c[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEX7GM2sWZgI",
    "outputId": "bd32e430-4cc3-498c-db22-70bd812d9cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "《三国演义》（精校版全本）作者：罗贯中\n",
      " \n",
      "\n",
      "内容简介\n",
      "\n",
      "　　《三国演义》由东汉末年黄巾起义末期开始\n",
      "\n",
      "Encoded:\n",
      "[16, 25, 655, 2054, 61, 17, 3946, 2583, 1732, 2143, 274, 1657, 3947, 174, 2734, 3949, 2694, 3249, 48, 0, 1, 0, 0, 292, 886, 2548, 118, 0, 0, 13, 13, 16, 25, 655, 2054, 61, 17, 2270, 40, 1876, 1656, 1039, 3919, 1005, 3290, 61, 1656, 1652, 1082, 796]\n",
      "\n",
      "Decoded:\n",
      "《三国演义》（精校版全本）作者：罗贯中\n",
      " \n",
      "\n",
      "内容简介\n",
      "\n",
      "　　《三国演义》由东汉末年黄巾起义末期开始\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\")\n",
    "print(text[:50])\n",
    "\n",
    "print(\"\\nEncoded:\")\n",
    "print(encoder(text[:50]))\n",
    "\n",
    "print(\"\\nDecoded:\")\n",
    "print(decoder(encoder(text[:50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkWkfGCglYnE",
    "outputId": "e29b65e2-8a2d-4742-8b7d-5449a8342e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters\n",
    "block_size = 192\n",
    "batch_size = 16\n",
    "d_model = 384\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.0\n",
    "lr_rate = 1e-3\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "eval_iters = 20\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vRIUm4Lf-YM",
    "outputId": "9928a69e-43d1-434f-93ff-a3b19fcb9c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([606051]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "# data = encoder(text)\n",
    "\n",
    "# Split up into training and validation sets.\n",
    "# Generate a random permutation of the entire dataset.\n",
    "# Sequence of length <block_size> is used to predict the next token\n",
    "# The last seq will be [len(data) - block_size - 1, len(data) - 2] (inclusive).\n",
    "# The last next token to be predicted will be <len(data) - 1>.\n",
    "# So the index won't be out of bound.\n",
    "perm = torch.randperm(len(data) - block_size)\n",
    "# Then first 90% are training data, and rest are for validation.\n",
    "n = int(0.9 * len(perm))\n",
    "# We only save the start position of each example instead of the entire\n",
    "# sequence. The sequence will be generated when creating the batches.\n",
    "train_indices = perm[:n]\n",
    "val_indices = perm[n:]\n",
    "\n",
    "# This won't work - consumes too much memory\n",
    "# training_set = [data[perm[i]:perm[i]+T] for i in perm[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rk8TjMqXcS2E"
   },
   "outputs": [],
   "source": [
    "# My simple dataloader. TODO: Can we use DataLoader instead?\n",
    "def get_batch(split):\n",
    "    # select training or validation set\n",
    "    indices = train_indices if split == 'train' else val_indices\n",
    "    # train_indices/val_indices stores the start locations\n",
    "    # pick <batch_size> number of them from the array\n",
    "    ix = torch.randint(len(indices), (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "S2jkJ810irts"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "  \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(d_model, head_size, bias=False)\n",
    "    self.query = nn.Linear(d_model, head_size, bias=False)\n",
    "    self.value = nn.Linear(d_model, head_size, bias=False)\n",
    "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B,T,C = x.shape\n",
    "    k = self.key(x)   # (B,T,C)\n",
    "    q = self.query(x) # (B,T,C)\n",
    "    # compute attention scores (\"affinities\")\n",
    "    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "    # apply the \"causal mask\"\n",
    "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "    wei = self.dropout(wei)\n",
    "    # perform the weighted aggregation of the values\n",
    "    v = self.value(x) # (B,T,C)\n",
    "    out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "  def __init__(self, num_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "    self.proj = nn.Linear(d_model, d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "    out = self.dropout(self.proj(out))\n",
    "    # print(\"MHA output shape\", out.shape)\n",
    "    return out # (B, T, n_embed)\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "  def __init__(self, d_model):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(d_model, 4 * d_model),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * d_model, d_model),\n",
    "        nn.Dropout(dropout),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "  def __init__(self, d_model, n_head):\n",
    "    # d_model: embedding dimension, n_head: the number of heads we'd like\n",
    "    super().__init__()\n",
    "    head_size = d_model // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size)\n",
    "    self.ffwd = FeedFoward(d_model)\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.sa(self.ln1(x))\n",
    "    x = x + self.ffwd(self.ln2(x))\n",
    "    # print(\"Block output shape\", x.shape)\n",
    "    return x   # (B, T, n_embed)\n",
    "\n",
    "# super simple bigram model\n",
    "class SanGuoGPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
    "        self.blocks = nn.Sequential(*[Block(d_model, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(d_model) # final layer norm\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTOOVQb0l0CD",
    "outputId": "d24f4821-61b3-44aa-da3e-be697427ca4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.753456 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = SanGuoGPTModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tdPDwwWGme-o"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "  out = {}\n",
    "  # set the model in evaluation mode\n",
    "  model.eval()\n",
    "  for split in ['train', 'val']:\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "      X, Y = get_batch(split)\n",
    "      logits, loss = model(X, Y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  # switch back to training mode\n",
    "  model.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CCNj-MTmQTr",
    "outputId": "5924d761-e05b-403c-92cb-f108617df1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 99: train loss 5.1277, val loss 5.3283\n",
      "step 199: train loss 4.7032, val loss 4.8544\n",
      "step 299: train loss 4.4147, val loss 4.6278\n",
      "step 399: train loss 4.2018, val loss 4.3512\n",
      "step 499: train loss 4.0151, val loss 4.1398\n",
      "step 599: train loss 3.8234, val loss 4.0403\n",
      "step 699: train loss 3.7339, val loss 3.8090\n",
      "step 799: train loss 3.5598, val loss 3.7358\n",
      "step 899: train loss 3.4869, val loss 3.5947\n",
      "step 999: train loss 3.3062, val loss 3.4453\n",
      "step 1099: train loss 3.2347, val loss 3.4102\n",
      "step 1199: train loss 3.1301, val loss 3.2392\n",
      "step 1299: train loss 2.9740, val loss 3.0964\n",
      "step 1399: train loss 2.8543, val loss 3.0026\n",
      "step 1499: train loss 2.8045, val loss 2.8930\n",
      "step 1599: train loss 2.5893, val loss 2.8269\n",
      "step 1699: train loss 2.5338, val loss 2.6030\n",
      "step 1799: train loss 2.4659, val loss 2.6086\n",
      "step 1899: train loss 2.3565, val loss 2.4526\n",
      "step 1999: train loss 2.2901, val loss 2.3496\n",
      "step 2099: train loss 2.1801, val loss 2.2719\n",
      "step 2199: train loss 2.0824, val loss 2.1367\n",
      "step 2299: train loss 1.9192, val loss 2.0006\n",
      "step 2399: train loss 1.8275, val loss 1.8758\n",
      "step 2499: train loss 1.7043, val loss 1.8304\n",
      "step 2599: train loss 1.6661, val loss 1.7581\n",
      "step 2699: train loss 1.5652, val loss 1.6762\n",
      "step 2799: train loss 1.4466, val loss 1.5084\n",
      "step 2899: train loss 1.3600, val loss 1.4000\n",
      "step 2999: train loss 1.3613, val loss 1.3337\n",
      "step 3099: train loss 1.2208, val loss 1.2715\n",
      "step 3199: train loss 1.1811, val loss 1.1910\n",
      "step 3299: train loss 1.0988, val loss 1.1132\n",
      "step 3399: train loss 1.0196, val loss 1.0799\n",
      "step 3499: train loss 0.9742, val loss 0.9592\n",
      "step 3599: train loss 0.9193, val loss 0.8814\n",
      "step 3699: train loss 0.8669, val loss 0.8524\n",
      "step 3799: train loss 0.7663, val loss 0.8619\n",
      "step 3899: train loss 0.7627, val loss 0.8015\n",
      "step 3999: train loss 0.7199, val loss 0.7167\n",
      "step 4099: train loss 0.6966, val loss 0.6518\n",
      "step 4199: train loss 0.6466, val loss 0.6732\n",
      "step 4299: train loss 0.6333, val loss 0.6169\n",
      "step 4399: train loss 0.5996, val loss 0.5905\n",
      "step 4499: train loss 0.5974, val loss 0.5833\n",
      "step 4599: train loss 0.5616, val loss 0.5575\n",
      "step 4699: train loss 0.5157, val loss 0.5368\n",
      "step 4799: train loss 0.5099, val loss 0.5158\n",
      "step 4899: train loss 0.5077, val loss 0.5225\n",
      "step 4999: train loss 0.4757, val loss 0.5105\n",
      "step 5099: train loss 0.4831, val loss 0.4831\n",
      "step 5199: train loss 0.4476, val loss 0.4612\n",
      "step 5299: train loss 0.4395, val loss 0.4528\n",
      "step 5399: train loss 0.4327, val loss 0.4420\n",
      "step 5499: train loss 0.4235, val loss 0.4550\n",
      "step 5599: train loss 0.4148, val loss 0.4166\n",
      "step 5699: train loss 0.4133, val loss 0.4166\n",
      "step 5799: train loss 0.4014, val loss 0.3972\n",
      "step 5899: train loss 0.3955, val loss 0.4166\n",
      "step 5999: train loss 0.3840, val loss 0.4047\n",
      "step 6099: train loss 0.3857, val loss 0.3718\n",
      "step 6199: train loss 0.3798, val loss 0.3778\n",
      "step 6299: train loss 0.3855, val loss 0.3738\n",
      "step 6399: train loss 0.3549, val loss 0.3663\n",
      "step 6499: train loss 0.3479, val loss 0.3501\n",
      "step 6599: train loss 0.3655, val loss 0.3538\n",
      "step 6699: train loss 0.3552, val loss 0.3634\n",
      "step 6799: train loss 0.3575, val loss 0.3476\n",
      "step 6899: train loss 0.3511, val loss 0.3486\n",
      "step 6999: train loss 0.3585, val loss 0.3637\n",
      "step 7099: train loss 0.3393, val loss 0.3497\n",
      "step 7199: train loss 0.3306, val loss 0.3529\n",
      "step 7299: train loss 0.3313, val loss 0.3459\n",
      "step 7399: train loss 0.3405, val loss 0.3312\n",
      "step 7499: train loss 0.3223, val loss 0.3173\n",
      "step 7599: train loss 0.3311, val loss 0.3209\n",
      "step 7699: train loss 0.3065, val loss 0.3085\n",
      "step 7799: train loss 0.3232, val loss 0.3316\n",
      "step 7899: train loss 0.3094, val loss 0.3068\n",
      "step 7999: train loss 0.3116, val loss 0.3113\n",
      "step 8099: train loss 0.2949, val loss 0.3172\n",
      "step 8199: train loss 0.3059, val loss 0.3206\n",
      "step 8299: train loss 0.2982, val loss 0.3138\n",
      "step 8399: train loss 0.3065, val loss 0.3108\n",
      "step 8499: train loss 0.3079, val loss 0.3015\n",
      "step 8599: train loss 0.3067, val loss 0.2957\n",
      "step 8699: train loss 0.3076, val loss 0.3104\n",
      "step 8799: train loss 0.2977, val loss 0.3033\n",
      "step 8899: train loss 0.2946, val loss 0.3132\n",
      "step 8999: train loss 0.2902, val loss 0.3007\n",
      "step 9099: train loss 0.2933, val loss 0.2929\n",
      "step 9199: train loss 0.2864, val loss 0.2919\n",
      "step 9299: train loss 0.2819, val loss 0.2781\n",
      "step 9399: train loss 0.2870, val loss 0.2810\n",
      "step 9499: train loss 0.2801, val loss 0.2736\n",
      "step 9599: train loss 0.2837, val loss 0.2894\n",
      "step 9699: train loss 0.2796, val loss 0.2758\n",
      "step 9799: train loss 0.2652, val loss 0.2694\n",
      "step 9899: train loss 0.2862, val loss 0.2668\n",
      "step 9999: train loss 0.2792, val loss 0.2763\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_rate)\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "  # every once in a while evaluate the loss on train and val sets\n",
    "  if ((iter+1) % eval_interval) == 0 or iter == max_iters - 1:\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "  # sample a batch of data\n",
    "  xb, yb = get_batch('train')\n",
    "\n",
    "  # evaluate the loss\n",
    "  logits, loss = model(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decoder(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"sanguogpt-v0.1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "class SanGuoData:\n",
    "    def __init__(self, source = 'sanguo-utf8.txt', block_size = 192, training_set_ratio = 0.9):\n",
    "        self.source = source\n",
    "        self.block_size = block_size\n",
    "        self.training_set_ratio = training_set_ratio\n",
    "        self.text = None\n",
    "        self.chars = None\n",
    "        self.vocab_size = 0\n",
    "        self.c2i = None\n",
    "        self.i2c = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.data = None\n",
    "    \n",
    "    def ingest(self, gen_dataset=True, gen_token_map=True):\n",
    "        with open(self.source, 'r', encoding='utf-8') as f:\n",
    "            self.text = f.read()\n",
    "        print(f\"Length of text: {len(self.text)}\")   # 606051 Chinese characters\n",
    "        # print(self.text[:100])\n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        self.vocab_size = len(self.chars) \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "        # I don't plan to use a tokenizer for this.\n",
    "        # IMHO, a Chinese character is not a \"letter\". Instead it's more like\n",
    "        # a word or subword. So we should treat each Chinese character as a token.\n",
    "\n",
    "        # Turn each character into a number (index into the chars array)\n",
    "        # Map character to index.\n",
    "        self.c2i = {ch:i for i, ch in enumerate(self.chars)}\n",
    "        # Map index to character.\n",
    "        self.i2c = {i:ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "        # Given a string (sequence of characters), encode it into a sequence of indices.\n",
    "        self.encoder = lambda s: [self.c2i[c] for c in s]\n",
    "        # Given a sequence of indices, decode it back to the string\n",
    "        self.decoder = lambda l: ''.join([self.i2c[i] for i in l])\n",
    "\n",
    "        self.data = torch.tensor(self.encoder(self.text), dtype=torch.long)\n",
    "        # print(self.data.shape, self.data.dtype)\n",
    "\n",
    "        if gen_token_map:\n",
    "            self.save_token_map()\n",
    "\n",
    "        if gen_dataset:\n",
    "            self.gen_dataset()\n",
    "    \n",
    "    def save_token_map(self, c2i_file:str = 'c2i.json', i2c_file:str='i2c.json'):\n",
    "        with open(c2i_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.c2i, f)\n",
    "        with open(i2c_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.i2c, f)\n",
    "\n",
    "    def test_enc_dec(self):\n",
    "        print(\"Original text:\")\n",
    "        print(self.text[:50])\n",
    "\n",
    "        print(\"\\nEncoded:\")\n",
    "        print(self.encoder(self.text[:50]))\n",
    "\n",
    "        print(\"\\nDecoded:\")\n",
    "        print(self.decoder(self.encoder(self.text[:50])))\n",
    "\n",
    "    def gen_dataset(self):\n",
    "        # Split up into training and validation sets.\n",
    "        # Generate a random permutation of the entire dataset.\n",
    "        # Sequence of length <block_size> is used to predict the next token\n",
    "        # The last seq will be [len(data) - block_size - 1, len(data) - 2] (inclusive).\n",
    "        # The last next token to be predicted will be <len(data) - 1>.\n",
    "        # So the index won't be out of bound.\n",
    "        self.perm = torch.randperm(len(self.data) - self.block_size)\n",
    "        # Then first 90% are training data, and rest are for validation.\n",
    "        n = int(self.training_set_ratio * len(self.perm))\n",
    "        # We only save the start position of each example instead of the entire\n",
    "        # sequence. The sequence will be generated when creating the batches.\n",
    "        self.train_indices = self.perm[:n]\n",
    "        self.val_indices = self.perm[n:]\n",
    "        self.train_batchptr = 0\n",
    "        self.val_batchptr = 0\n",
    "    \n",
    "    # If `random` is True, we randomly pick batch_size items from the set.\n",
    "    # But since training_indices/val_indices are already shuffled, this is not really\n",
    "    # needed.\n",
    "    def get_batch(self, split:str, batch_size, device, random=False):\n",
    "        # select training or validation set\n",
    "        indices = self.train_indices if split == 'train' else self.val_indices\n",
    "        ptr = self.train_batchptr if split == 'train' else self.val_batchptr\n",
    "\n",
    "        # train_indices/val_indices stores the start locations\n",
    "        if random:\n",
    "            ix = torch.randint(len(indices), (batch_size,))\n",
    "        else:\n",
    "            # The train/val set is already shuffled, so we just need to sequentially\n",
    "            # go through the items batch by batch.\n",
    "            next = ptr + batch_size\n",
    "            if next < len(indices):\n",
    "                ix = indices[ptr:next]\n",
    "            else:\n",
    "                # Handle the case when we wrap around the list.\n",
    "                next = next % len(indices)\n",
    "                ix = torch.cat((indices[ptr:len(indices)], indices[0:next]))\n",
    "            # Move the batch pointer\n",
    "            if split == 'train':\n",
    "                self.train_batchptr = next\n",
    "            else:\n",
    "                self.val_batchptr = next\n",
    "        # Generate the actual examples & labels for the batch.        \n",
    "        x = torch.stack([self.data[i:i+self.block_size] for i in ix])\n",
    "        y = torch.stack([self.data[i+1:i+self.block_size+1] for i in ix])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def encoder(s:str, c2i:dict):\n",
    "    return [c2i[c] for c in s] \n",
    "\n",
    "def decoder(l, i2c:dict):\n",
    "    return ''.join([i2c[i] for i in l]) \n",
    "\n",
    "def load_token_map(c2i_file:str = 'c2i.json', i2c_file:str='i2c.json'):\n",
    "    # Load token map from the file.\n",
    "    with open('c2i.json', 'r', encoding='utf-8') as f:\n",
    "        c2i = json.load(f)\n",
    "\n",
    "    # When loaded from JSON, the keys will become strings (e.g. '3913': '麒' instead of 3913: '麒')\n",
    "    with open('i2c.json', 'r', encoding='utf-8') as f:\n",
    "        i2c_raw = json.load(f)\n",
    "        # Convert the keys to integers.\n",
    "        i2c = {int(i):i2c_raw[i] for i in i2c_raw.keys()} \n",
    "    \n",
    "    return c2i, i2c\n",
    "\n",
    "# Test load/save token map.\n",
    "def test_token_map():\n",
    "    data = SanGuoData()\n",
    "    data.ingest()\n",
    "    # Save the token map to json files.\n",
    "    data.save_token_map()\n",
    "\n",
    "    # Load the token map and test encoding/decoding with it.\n",
    "    c2i, i2c = load_token_map()\n",
    "\n",
    "    print(\"Original text:\")\n",
    "    print(data.text[50:100])\n",
    "\n",
    "    print(\"\\nEncoded:\")\n",
    "    print(encoder(data.text[50:100], c2i))\n",
    "\n",
    "    print(\"\\nDecoded:\")\n",
    "    print(decoder(encoder(data.text[50:100], c2i), i2c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# v0.1:\n",
    "# Super simple, hand-written Transformer model. \n",
    "# Inspired by the \"Building a GPT\" notebook by Andrej Karpathy.\n",
    "# https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, d_model, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # apply the \"causal mask\"\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, head_size, d_model, dropout, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size=head_size,\n",
    "                                        d_model=d_model,\n",
    "                                        block_size=block_size,\n",
    "                                        dropout=dropout) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        # print(\"MHA output shape\", out.shape)\n",
    "        return out # (B, T, n_embed)\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_head, dropout, block_size):\n",
    "        # d_model: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = d_model // n_head\n",
    "        self.sa = MultiHeadAttention(n_head=n_head,\n",
    "                                    head_size=head_size,\n",
    "                                    d_model=d_model,\n",
    "                                    dropout=dropout,\n",
    "                                    block_size=block_size)\n",
    "        self.ffwd = FeedFoward(d_model=d_model, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        # print(\"Block output shape\", x.shape)\n",
    "        return x   # (B, T, n_embed)\n",
    "\n",
    "# super simple model\n",
    "class SanGuoGPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_layer, dropout, block_size, n_head, device):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
    "        self.blocks = nn.Sequential(*[Block(d_model=d_model,\n",
    "                                            n_head=n_head,\n",
    "                                            dropout=dropout,\n",
    "                                            block_size=block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(d_model) # final layer norm\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layer = n_layer\n",
    "        self.dropout = dropout\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)   # loss is a scalar\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # perplexity = exp(-1/N sum(log(p(w_i|w_1,...,w_{i-1}))))\n",
    "        sum_log_p = torch.zeros(idx.shape[0])   # shape (B,)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # calculate perplexity: sum of log_p\n",
    "            log_p = torch.tensor([probs[i,idx_next[i,0].item()] for i in range(idx_next.shape[0])])\n",
    "            log_p = torch.log(log_p)\n",
    "            sum_log_p += log_p\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        # Return the generated text along with perplexity.\n",
    "        return idx, torch.exp(-1.0 * sum_log_p / max_new_tokens)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_embeddings(self, tokens):\n",
    "        return self.token_embedding_table(tokens)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.input = 'sanguo-utf8.txt'\n",
    "args.output = 'sanguogpt.pth'\n",
    "args.no_save_model = False\n",
    "args.batch_size = 32\n",
    "args.block_size = 256\n",
    "args.d_model = 384\n",
    "args.num_heads = 8\n",
    "args.num_layers = 6\n",
    "args.dropout = 0.01\n",
    "args.lr_rate = 6e-4\n",
    "args.min_lr = 6e-5\n",
    "args.lr_decay_iters = 60000\n",
    "args.warmup_iters = 1000\n",
    "args.decay_lr = True\n",
    "args.num_iters = 1000\n",
    "args.eval_interval = 100\n",
    "args.eval_iters = 10\n",
    "args.training_set_ratio = 0.9\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, data, device):\n",
    "    out = {}\n",
    "    eval_iters = args.eval_iters\n",
    "    batch_size = args.batch_size\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            # For validation, we randomly pick a few items as a batch.\n",
    "            X, Y = data.get_batch(split, batch_size=batch_size, device=device, random=True)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    # switch back to training mode\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train(session_name:str = None):\n",
    "    # Some hyperparameters\n",
    "    block_size = args.block_size\n",
    "    batch_size = args.batch_size\n",
    "    d_model = args.d_model\n",
    "    n_head = args.num_heads\n",
    "    n_layer = args.num_layers\n",
    "    dropout = args.dropout\n",
    "    lr_rate = args.lr_rate\n",
    "    max_iters = args.num_iters\n",
    "    eval_interval = args.eval_interval\n",
    "    training_set_ratio = args.training_set_ratio\n",
    "    device = (\n",
    "        \"cuda\" if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "\n",
    "    if session_name is None:\n",
    "        writer = None\n",
    "    else:\n",
    "        writer = SummaryWriter(os.path.join('runs', session_name))\n",
    "\n",
    "    # Prepare the dataset\n",
    "    sanguo_data = SanGuoData(source = args.input, block_size = block_size, training_set_ratio = training_set_ratio)\n",
    "    sanguo_data.ingest()\n",
    "    print(f\"Number of tokens in each batch: {block_size*batch_size}\")\n",
    "\n",
    "    # Create the model\n",
    "    model = SanGuoGPTModel(vocab_size=sanguo_data.vocab_size,\n",
    "                        d_model=d_model,\n",
    "                        n_layer=n_layer,\n",
    "                        dropout=dropout,\n",
    "                        block_size=block_size,\n",
    "                        n_head=n_head,\n",
    "                        device=device\n",
    "                        )\n",
    "    m = model.to(device)\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # Visualize the model\n",
    "    xb, yb = sanguo_data.get_batch('train', batch_size=batch_size, device=device, random=True)\n",
    "    if writer is not None:\n",
    "        writer.add_graph(model, (xb, yb))\n",
    "        writer.flush()\n",
    "\n",
    "    # For visualizing the embeddings:\n",
    "    # encoder returns a list for each string (which is a single character in vocabulary).\n",
    "    # Therefore, the shape of all_token will be like (vocab_size, 1).\n",
    "    # We want a 1-D list, so we squeeze the last dimension and change the shape to (vocab_size, ).\n",
    "    all_tokens = torch.tensor([sanguo_data.encoder(ch) for ch in sanguo_data.chars[20:-7]],\n",
    "                            dtype=torch.long, device=device, requires_grad=False).squeeze(1)\n",
    "    print('tokens for visualization', all_tokens.shape)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_rate)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    training_start = time.time()\n",
    "    # Training loop\n",
    "    for iter in range(max_iters):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if (iter % eval_interval) == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model, sanguo_data, device)\n",
    "            # perplexity = exp(cross_entropy)\n",
    "            ppl_train = math.exp(losses['train'].item())\n",
    "            ppl_val = math.exp(losses['val'].item())\n",
    "            print(f\"step {iter}: train loss {losses['train']:.3f}, perplexity {ppl_train:.3f}, val loss {losses['val']:.3f}, perplexity {ppl_val:.3f}\")\n",
    "            # Log the estimated training loss and validation loss\n",
    "            if writer is not None:\n",
    "                writer.add_scalars(\n",
    "                    # main_tag\n",
    "                    'Estimated Training vs. Validation Loss',\n",
    "                    # tag_scalar\n",
    "                    {\n",
    "                        'Training': losses['train'].item(),\n",
    "                        'Validation': losses['val'].item(),\n",
    "                    },\n",
    "                    # global_step\n",
    "                    iter)\n",
    "                writer.add_scalars(\n",
    "                    # main_tag\n",
    "                    'Estimated Training vs. Validation Perplexity',\n",
    "                    # tag_scalar\n",
    "                    {\n",
    "                        'Training': ppl_train,\n",
    "                        'Validation': ppl_val,\n",
    "                    },\n",
    "                    # global_step\n",
    "                    iter)\n",
    "                # Visualize the embeddings.\n",
    "                embedding_table = model.get_embeddings(all_tokens)  # (vocab_size, d_model)\n",
    "                # print(embedding_table.shape)\n",
    "                writer.add_embedding(embedding_table, metadata=sanguo_data.chars[20:-7], tag=f\"embeddings-step{iter}\")\n",
    "                writer.flush()\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = sanguo_data.get_batch('train', batch_size=batch_size, device=device)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if (iter % eval_interval) == 0 or iter == max_iters - 1:\n",
    "        #     loss_val = loss.item()\n",
    "        #     print(f\"iteration: {iter:>6d}, loss: {loss_val:>7f}\")\n",
    "\n",
    "    training_end = time.time()\n",
    "    print(\"Finished training\")\n",
    "    print(f\"Total number of tokens trained: {block_size*batch_size*max_iters}\")\n",
    "    print(f\"Time elapsed: {training_end-training_start:.3f} seconds\")\n",
    "    print(f\"Training throughput: {(block_size*batch_size*max_iters)/(training_end-training_start):>.3f} tokens/sec\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_iters = 1000\n",
    "m = train(session_name='test-embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints/sanguogpt-v0.2.2.pth\n",
      "Loading token map file from c2i.json and i2c.json\n",
      "Using mps device\n",
      "SanGuoGPTModel(\n",
      "  (token_embedding_table): Embedding(3952, 384)\n",
      "  (position_embedding_table): Embedding(256, 384)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.001, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.001, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.001, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.001, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.001, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.001, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.001, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.001, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.001, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.001, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.001, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.001, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.001, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.001, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.001, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (key): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=48, bias=False)\n",
      "            (dropout): Dropout(p=0.001, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.001, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.001, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=384, out_features=3952, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# parser.add_argument('-m', '--model', default='checkpoints/sanguogpt-v0.2.pth', help='Input text for training.', type=str)\n",
    "# parser.add_argument('-l', '--gen_length', default=100, help='Maximum length to generate.', type=int)\n",
    "# parser.add_argument('--c2i', default='c2i.json', help='Token map file (character to index).', type=str)\n",
    "# parser.add_argument('--i2c', default='i2c.json', help='Token map file (index to character).', type=str)\n",
    "# parser.add_argument('--prompt', default=' ', help='Prompt for text generation.', type=str)\n",
    "# parser.add_argument('--webui', action='store_true', help='If specified, use streamlit-based Web UI instead of command line.')\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.model = 'checkpoints/sanguogpt-v0.2.2.pth'\n",
    "args.gen_length = 100\n",
    "args.c2i = 'c2i.json'\n",
    "args.i2c = 'i2c.json'\n",
    "args.prompt = ' '\n",
    "args.webui = False\n",
    "\n",
    "print(f\"Loading model from {args.model}\")\n",
    "model = torch.load(args.model)\n",
    "\n",
    "c2i, i2c = load_token_map(c2i_file=args.c2i, i2c_file=args.i2c)\n",
    "print(f\"Loading token map file from {args.c2i} and {args.i2c}\")\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens generated in 4.266 seconds, avg 23.444 tokens/sec.\n",
      "Perplexity of generation: 1.0218\n",
      "关羽次之，张飞为弟。祭罢天地，复宰牛设酒，聚乡中勇士，得三百余人，就桃园中痛饮一醉。来日收拾军器，但恨无马匹可乘。正思虑间，人报有两个客人，引一伙伴当，赶一群马，投庄上来。玄德曰：“此天佑我也！”三人出庄\n"
     ]
    }
   ],
   "source": [
    "def gen_response(prompt:str) -> str:\n",
    "    start = time.time()\n",
    "    # encode the prompt into a tensor, and reshape it into (1, T)\n",
    "    # first dimension is the batch, which is expected by the forward method.\n",
    "    context = torch.tensor(encoder(prompt, c2i), device=device).unsqueeze(0)\n",
    "    # model.generate() will truncate the prompt if it's too long, no need to worry about this.\n",
    "    resp_idx, ppl = model.generate(context, max_new_tokens=args.gen_length) \n",
    "    resp = decoder(resp_idx[0].tolist(), i2c)\n",
    "    end = time.time()\n",
    "    tokens_generated = min(args.gen_length, len(resp) - len(prompt))\n",
    "    print(f\"{tokens_generated} tokens generated in {end-start:>.3f} seconds, avg {tokens_generated/(end-start):>.3f} tokens/sec.\")\n",
    "    print(f\"Perplexity of generation: {ppl[0].item():>.4f}\")\n",
    "    return resp\n",
    "\n",
    "args.prompt = '关羽'\n",
    "print(gen_response(args.prompt))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
